# stablelm-zephyr-serverless

## Overview
This project is a chatbot service built using FastAPI, powered by the llama-cpp library. It uses a pre-trained model from **Hugging Face** `TheBloke/stablelm-zephyr-3b-GGUF` to generate chat completions based on user input. 

The service is designed to be deployed on Google Cloud Run, offering a scalable and serverless platform for handling requests, and runs fully on CPU thanks to the efficient inference of [llama-cpp](https://github.com/ggerganov/llama.cpp).

The LLM Running is StableLM Zephyr 3B, which is a 3 billion parameters models released by [Stability.ai](https://stability.ai/news/stablelm-zephyr-3b-stability-llm).

!NB: This model is released under a non-commercial license, and is not suitable for commercial use. You can contact Stability.ai for commercial licensing [here](https://stability.ai/contact).

## Google Cloud Platform (GCP) Configuration
### Prerequisites
**Google Cloud SDK**: Ensure you have the Google Cloud SDK installed and configured for your GCP account.
**Enable Cloud Build**, **Cloud Secret API**, **Artifact Registry** and **Cloud Run APIs**: Make sure the APIs for Cloud Build and Cloud Run are enabled in your GCP project.
Cloud Build.
**Create a repository in Artifact Registry**: Create a repository in Artifact Registry to store the Docker image.

**Create a Cloud Build Trigger**: Create a Cloud Build trigger that will build and deploy the service on every push to the main branch of the repository (you will need to conenct your GitHub account to GCP). The trigger should use the cloudbuild.yaml file in the repository.

Here is the configuration for the trigger I have used and that seems to work well:

- **_CONCURRENCY_**: 1
- **_CPU_**: 8
- **_MEMORY_**:  4G
- **_PROJECT**: [YOUR_PROJECT_ID]
- **_REGION**: [YOUR_REGION]
- **_REPO_NAME**: [YOUR_ARTIFACT_REGISTRY_REPO_NAME]
- **_SERVICE_NAME**: [YOUR_CLOUD_RUN_SERVICE_NAME]





Redirects to the FastAPI autogenerated documentation (/docs).
GET /health

Health check endpoint. Returns {"status": "ok"} if the service is running.
GET /complete

Generates a chat completion.
Parameters:
user (required): User's message input.
system: The system message, defaulting to "You are a story writing assistant."
temperature: Controls the randomness of the response, defaulting to 0.7.
Example Request
bash
Copy code
curl -X 'GET' \
  'https://[CLOUD_RUN_SERVICE_URL]/complete?user=Hello%20there'
# Local Setup
To run the service locally, ensure you have Docker installed. Build the Docker image using the provided Dockerfile and run it, because I use openblas for CPU inference, you might need to run it with the x86_64 architecture if you are on a apple silicon machine.
docker build -t chatbot-service .
docker run -p 8000:8000 chatbot-service
Access the service at http://localhost:8000.